{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d983d8b9",
   "metadata": {},
   "source": [
    "# About the notebook\n",
    "This notebook is designed to train a deep learning model for classificating 2D input image (C,W,H). \n",
    "\n",
    "The notebook is intended to be user-friendly, intuitive and does not require any programming skills to train the model. The user only needs to provide a training set consisting of input images and their corresponding target masks (also called ground truth images). The input images represent the images that will be segmented, while the target images contain the desired segmentation mask for each input image.\n",
    "\n",
    "**Training dataset requirements:** To use the notebook, the user needs to provide the paths of the folders containing the images to classify. **Please note that the current version of the notebook only accepts images in the TIF file format**. \n",
    "\n",
    "<br> *The input image* in each folder is required to be in a specific shape, which is \\[C1,H,W\\], where C1 is the number of channels, and H and W are the height and width of the image, respectively. This means that if the input image contains multiple channels, such as a RGB image, the channels should be included in a single file in the specified shape. \n",
    "\n",
    "Once the model is trained, the notebook \"Predict_Using_Model.ipynb\" can be used to generate the label corresponding to the classification for new input images. This can be done by providing a new set of input images and running the trained model on these images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671fcf9f",
   "metadata": {},
   "source": [
    "# 00 - Special Instructions for Google Colab Users \n",
    "\n",
    "The following lines of code should be executed only when running your script on Google Colab. This is crucial to leverage the additional features provided by Colab, most notably, the availability of a free GPU.  **If, you're running the code locally, this line can be skipped (GO TO STEP 01 - Loading dependencies) as it pertains specifically to the Colab setup.**\n",
    "\n",
    "## Give access to google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8cbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a226f0",
   "metadata": {},
   "source": [
    "## Install Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install napari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e16485",
   "metadata": {},
   "source": [
    "## Copy code to current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a08a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/paul-hernandez-herrera/image_classification_pytorch\n",
    "import os\n",
    "workbookDir = \"/content/image_classification_pytorch/\"\n",
    "os.chdir(workbookDir)\n",
    "\n",
    "# workaround para evitar error de pytorch... Need to fix this \n",
    "!export LC_ALL=\"en_US.UTF-8\"\n",
    "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "!ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415aac98",
   "metadata": {},
   "source": [
    "# 01 - Loading dependencies\n",
    "In this notebook, before running any code, there are several libraries and modules that need to be imported to ensure that the notebook runs smoothly. These libraries and modules contain pre-written code that performs specific tasks, such as reading and processing images, defining the UNET model, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'workbookDir' not in globals():\n",
    "    print('Updating working directory')\n",
    "    workbookDir = os.path.dirname(os.getcwd())\n",
    "    os.chdir(workbookDir)\n",
    "print(os.getcwd())\n",
    "\n",
    "from core_code.util.deeplearning_util import get_model_outputdir, get_batch_size, get_model, calculate_test_performance, load_model, get_dataloader_file_names\n",
    "from core_code.util.show_image import show_images_from_Dataset\n",
    "from core_code.util.util import write_list_to_file\n",
    "from core_code.Dataset.Dataset import CustomImageDataset\n",
    "from core_code.train import train_model\n",
    "from core_code.parameters_interface.parameters_widget import parameters_model_training_classification, parameters_folder_path ,parameters_training_images, parameters_data_augmentation\n",
    "from core_code.parameters_interface.ipwidget_basic import set_Int\n",
    "from core_code.parameters_interface import options\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim, nn, cuda\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33824320",
   "metadata": {},
   "source": [
    "# 02 - Setting required paths of training set\n",
    "In this section, the user can specify the paths to the training set by specification the folders containing the input images for each class. This is done by setting parameter: \"Folder path input images\" \n",
    "\n",
    "## Required parameters\n",
    "**Folder path - Class i**: the path of the folder containing the input images corresponding to class i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc4846",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_training_set = parameters_training_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427da359",
   "metadata": {},
   "source": [
    "# 03 - Visualizing samples from training set\n",
    "This section is designed to help the user gain a better understanding of the input images in the training set. By visualizing these samples, the user can confirm that the images are properly paired with the appropiate class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CustomImageDataset(parameters_training_set.get()[\"folder_input_list\"])\n",
    "print('Number of samples in training set: ' + str(len(training_dataset)))\n",
    "show_images_from_Dataset(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251778fb",
   "metadata": {},
   "source": [
    "# 04 - Data augmentation\n",
    "Data augmentation is a technique commonly used in machine learning to increase the size and variability of a training set by applying various transformations to the original data. These transformations may include flips, rotations, zooms, and shears, among others. The goal of data augmentation is to provide the model with additional training data that captures different variations of the same object, thereby improving the model's ability to generalize to new datasets.\n",
    "\n",
    "In cases where the training set is relatively small or contains limited variations, data augmentation can be especially useful. By increasing the variability of the training set, data augmentation helps to prevent the model from overfitting to the limited training data and producing inaccurate results on new datasets.\n",
    "\n",
    "In this notebook, users have the option to enable or disable data augmentation by selecting the appropriate flag. When disabled, the original training images are used without any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e608bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_augmentation = parameters_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d9b68",
   "metadata": {},
   "source": [
    "## Visualizing samples of augmented images\n",
    "After enabling data augmentation in the notebook, users may want to visualize how the transformed images look like. The \"Display sample of augmented images\" section provides users with an option to visualize the augmented images in order to assess the effectiveness of the data augmentation process. By visualizing these images, users can determine if the data augmentation process is appropriate and if it is achieving the desired variability in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa40f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if parameters_augmentation.get()[\"enable_data_augmentation\"]:\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"enable_data_augmentation\"], \n",
    "                                           options.get_data_augmentation(parameters_augmentation.get()))\n",
    "    show_images_from_Dataset(training_dataset)\n",
    "else:\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"enable_data_augmentation\"])\n",
    "    print('Data augmentation disable')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493bc9c",
   "metadata": {},
   "source": [
    "# 05 - Setting  important parameters\n",
    "Deep learning models consists of millions of parameters that need to be optimized to achieve accurate results.\n",
    "During the training process, the model learns to optimize its parameters to minimize a loss function, which measures the difference between the predicted output and the ground truth. The optimization process requires defining several important parameters, including: \n",
    "<br>**number of epochs:**  This parameter specifies the number of times the entire dataset is processed during the training process. Setting an appropriate number of epochs  is important to ensure that the model converges to an optimal solution\n",
    "<br>**validation:** This parameter specifies a separate dataset that is used to evaluate the performance of the model during the training process. The validation dataset is typically used to monitor the progress of the model.\n",
    "<br>**loss function:** This parameter specifies the function used to measure the difference between the predicted outputs and the ground truth labels.\n",
    "<br>**optimizer:** This parameter specifies the optimization algorithm used to update the model parameters during training. \n",
    "<br>**Device:** *Select the device CPU or GPU (if available)*. The fact that the U-Net model consists of millions of parameters means that the optimization process can be computationally expensive and time-consuming. However, with the advent of specialized hardware, such as GPUs and TPUs, deep learning practitioners can accelerate the optimization process and train large models like U-Net in a reasonable amount of time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532fc02",
   "metadata": {},
   "source": [
    "## Setting parameters required to train the model\n",
    "You can set all the previously describe parameters using the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _  = training_dataset.__getitem__(0)\n",
    "n_channels_input  = img.shape[0]\n",
    "n_classes = training_dataset.__n_classes__()\n",
    "\n",
    "parameters_model = parameters_model_training_classification(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a8b64",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "<br>**batch size:** the number of images to be use per iteration to compute the gradient.  It is often recommended to use the largest batch size possible that can fit in the available memory without causing an out-of-memory error.\n",
    "\n",
    "**We provide the following batch size for your computer specifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _  = training_dataset.__getitem__(0)\n",
    "batch_size = get_batch_size(model_type = 'resnet50',\n",
    "                            device = parameters_model.get('device'),\n",
    "                            input_shape = (img.shape[0], img.shape[1],img.shape[2]),\n",
    "                            output_shape= (training_dataset.__n_classes__(),),\n",
    "                            dataset_size= len(training_dataset),\n",
    "                            max_batch_size = 32\n",
    "    )\n",
    "print('------------------------------')\n",
    "print('\\033[41m' '\\033[1m' 'RECOMMENDED BATCH SIZE' '\\033[0m')\n",
    "print('You can set a lower value but not a higher one. A value equal to zero means that your device does not have enough memory for training, and you need to select another device.')\n",
    "print('------------------------------')\n",
    "batch_size_w = set_Int(\"batch_size:\", batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cb966",
   "metadata": {},
   "source": [
    "# 05 - Train the classification model (ResNet50)\n",
    "This function is the main routine for optimizing the parameters of the ResNet50 model, using the hyperparameters that are \n",
    "specified by the user \n",
    "\n",
    "## Run training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this parameters sets the number of models to generate\n",
    "NUM_RUNS = 1\n",
    "print('------------------------------')\n",
    "print('\\033[42m' '\\033[1m' 'PARAMETERS VALUES    ' '\\033[0m')\n",
    "print('------------------------------')\n",
    "print(f'Number of samples in training set: {len(training_dataset)}')\n",
    "p = parameters_model.get(\"device\")\n",
    "print(f\"Device: {p}\")\n",
    "p = parameters_model.get(\"model_output_folder\")\n",
    "print(f\"model_output_folder: {p}\")\n",
    "print(f\"batch_size: {batch_size_w.value}\")\n",
    "p = parameters_model.get(\"epochs\")\n",
    "print(f\"Epochs: {p}\")\n",
    "p = parameters_augmentation.get()\n",
    "print(f\"data augmentation: {p}\")\n",
    "p = parameters_model.get(\"loss_function\")\n",
    "print(f\"loss_function: {p}\")\n",
    "p = parameters_model.get(\"lr_scheduler_par\")\n",
    "print(f\"lr_scheduler_par: {p}\")\n",
    "p = parameters_model.get(\"validation_par\")\n",
    "print(f\"validation_par: {p}\")\n",
    "p = parameters_model.get(\"test_par\")\n",
    "print(f\"test_par: {p}\")\n",
    "print('------------------------------')\n",
    "print('------------------------------')\n",
    "\n",
    "for i in range(0,NUM_RUNS):\n",
    "    start = time.time()\n",
    "    \n",
    "    #getting batch size\n",
    "    batch_size = batch_size_w.value\n",
    "\n",
    "    # getting device to perform operations (recommended to use GPU)\n",
    "    device = parameters_model.get('device')\n",
    "\n",
    "    #construct the model\n",
    "    model = get_model('resnet50', n_channels_input, n_classes)\n",
    "    model.to(device= device)\n",
    "    print('------------------------------')\n",
    "    print('Creating a ResNet50 model. It receives images with ' + str(n_channels_input) + ' channels and predicts a class label with ' + str(n_classes) + ' classes')\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('The ResNet50 model have ' + \"{:,}\".format(trainable_params) + ' parameters to optimize.')\n",
    "    print('------------------------------')\n",
    "    \n",
    "    #setting the values for data augmentation\n",
    "    data_augmentation_object = options.get_data_augmentation(parameters_augmentation.get())\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"enable_data_augmentation\"], data_augmentation_object)\n",
    "\n",
    "    #creating data loaders\n",
    "    new_training_dataset, validation_dataset, test_dataset = options.get_split_training_val_test_sets(\n",
    "        training_dataset,\n",
    "        parameters_model.get(\"validation_par\"), \n",
    "        parameters_model.get(\"test_par\"))\n",
    "\n",
    "    train_loader = DataLoader(new_training_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size = batch_size) if validation_dataset else None\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size) if test_dataset else None\n",
    "\n",
    "    #getting the criterion to be use to measure the performance of the model to predict target data\n",
    "    loss_functions = options.get_loss_function(parameters_model.get(\"loss_function\"))\n",
    "\n",
    "    # getting the optimizer to update weight\n",
    "    optimizer = options.get_optimizer(option_name = parameters_model.get(\"optimizer_par\")[\"Optimizer\"],\n",
    "                                     model = model,\n",
    "                                     lr = parameters_model.get(\"optimizer_par\")[\"lr\"],\n",
    "                                     weight_decay = parameters_model.get(\"optimizer_par\")[\"weight_decay\"],\n",
    "                                     momentum = parameters_model.get(\"optimizer_par\")[\"momentum\"],\n",
    "                                     betas = parameters_model.get(\"optimizer_par\")[\"betas\"])\n",
    "\n",
    "    lr_scheduler = options.get_lr_scheduler(**parameters_model.get(\"lr_scheduler_par\"), optimizer = optimizer)\n",
    "\n",
    "    # number of iterations for each image in the training set\n",
    "    epochs = parameters_model.get(\"epochs\")\n",
    "\n",
    "    #parameters model_output\n",
    "    output_dir = parameters_model.get(\"model_output_folder\")\n",
    "    if NUM_RUNS>1:\n",
    "        output_dir = Path(output_dir, f\"RUN_{i}\")\n",
    "    save_checkpoint = parameters_model.get(\"model_checkpoint\")\n",
    "    checkpoint_frequency = parameters_model.get(\"model_checkpoint_frequency\")\n",
    "    \n",
    "    model = train_model(model = model,\n",
    "                        train_loader = train_loader,\n",
    "                        validation_loader = validation_loader,\n",
    "                        loss_functions = loss_functions,\n",
    "                        optimizer = optimizer,\n",
    "                        epochs = epochs,\n",
    "                        device = device,\n",
    "                        output_dir = output_dir,\n",
    "                        save_checkpoint = save_checkpoint,\n",
    "                        checkpoint_frequency = checkpoint_frequency,\n",
    "                        lr_scheduler = lr_scheduler\n",
    "                       )\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"elapse time for training model {i}: {end - start} seconds\")\n",
    "\n",
    "    write_list_to_file(Path(output_dir,'train_dataset.txt'), get_dataloader_file_names(train_loader))    \n",
    "    if test_dataset:\n",
    "        calculate_test_performance(model, test_loader, device, folder_output = output_dir)\n",
    "        write_list_to_file(Path(output_dir,'test_dataset.txt'), get_dataloader_file_names(test_loader))\n",
    "    \n",
    "    if validation_dataset:\n",
    "        write_list_to_file(Path(output_dir,'validation_dataset.txt'), get_dataloader_file_names(validation_loader))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aadced",
   "metadata": {},
   "source": [
    "# 07 [Optional] -  Evaluate performance of the model\n",
    "After training the model, it's important to evaluate its performance in terms of how well it can map input images to the desired ground-truth images. One way to measure performance is through the loss function, which should be reducing values as training progresses. This means that the model's error in predicting similar images to the ground truth is also reducing. If you have a validation set, the error on that set should also be decreasing over time.\n",
    "\n",
    "For a more detailed explanation of how to interpret training and validation loss in assessing model performance, you can refer to the following webpage: [Training and validation loss explanation](https://www.baeldung.com/cs/training-validation-loss-deep-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tensorboard = Path(output_dir, 'tensorboard').as_posix()\n",
    "%tensorboard --logdir=$folder_tensorboard  --port=6006 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713f4a2",
   "metadata": {},
   "source": [
    "# 08 [Optional] - Testing the Trained Model\n",
    "After completing the training process, the trained model can now be used to predict masks for new and unseen data. To achieve this, we recommend using the Jupyter notebook **\"predict_using_trained_model.ipynb\"**.\n",
    "\n",
    "It's important to note that this section is only applicable if the model has been trained (step 06). As such, to avoid any confusion, we highly recommend using the provided Jupyter notebook to make predictions, as it doesn't require training a new model.\n",
    "\n",
    "you'll need to define the following parameters to test the trained model:\n",
    "<br>**Folder or file path**: Folder or file path containing the input images \n",
    "<br>**Folder output**: the folder where the model's output will be saved. If no folder output is specified, a folder named 'output' will be created in the folder containing the input images to save the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_test_set = parameters_folder_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a59163",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_test_path, folder_output_test = parameters_test_set.get()\n",
    "predict_model(model, folder_test_path, folder_output_test, device = device);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
